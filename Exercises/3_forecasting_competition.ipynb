{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# University Certificate in Artificial Intelligence (Hands on AI, Third Challenge, 2022-2023, UMONS)\n",
        "# Kaggle competition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJKLhOe89jlb"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPdAdruJtE2t"
      },
      "outputs": [],
      "source": [
        "# The dataset and package \"main\" are in the GitHub repository\n",
        "!git clone https://github.com/bsouhaib/Hands-On-AI-2022-Challenge3.git\n",
        "%cd Hands-On-AI-2022-Challenge3/Exercises\n",
        "# These packages are not installed by default\n",
        "!pip install pmdarima supersmoother"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "am6qFBiQsf7v"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "import os, glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fc5mZjZEsf7w"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"../work\"):\n",
        "    os.makedirs(\"../work\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K-ymse8sf7x"
      },
      "outputs": [],
      "source": [
        "# From Dataframe (HORIZON X SERIES) to Kaggle format (s001h3, ...)\n",
        "def kaggle_forecasts(fcts):\n",
        "    submission = fcts.copy()\n",
        "    submission.index = 1 + np.arange(len(submission))\n",
        "    submission = submission.stack()\n",
        "    submission.name = \"Forecasts\"\n",
        "    submission = submission.reset_index()\n",
        "\n",
        "    submission[\"Id\"] = submission[\"level_1\"] + \"h\" + submission[\"level_0\"].apply(str)\n",
        "    submission.drop([\"level_0\", \"level_1\"], axis=1, inplace=True)\n",
        "    submission = submission[[\"Id\", \"Forecasts\"]]\n",
        "    return submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855hMxyMFaVV"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUVuSvyxsf7x"
      },
      "outputs": [],
      "source": [
        "DT_train = pd.read_csv(\"../data/public/train.csv\", parse_dates=True)\n",
        "DT_train[\"Day\"] = pd.to_datetime(DT_train[\"Day\"], format=\"%Y-%m-%d\")\n",
        "DT_train.set_index(\"Day\", inplace=True)\n",
        "DT_train = DT_train.asfreq(\"D\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0cIxnCNsf7x"
      },
      "outputs": [],
      "source": [
        "# Filling missing values\n",
        "DT_train.fillna(method=\"backfill\", inplace=True)\n",
        "DT_train.isna().any().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J79hmO-vFZFa"
      },
      "outputs": [],
      "source": [
        "HORIZON = 7 * 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z44oJZ27fdl6"
      },
      "source": [
        "`forecast_for_kaggle` has to be set to True for the Kaggle competition.\n",
        "Setting it to False allows to have access to a test dataset using data from `DT_train`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_MMlOr7sf7x"
      },
      "outputs": [],
      "source": [
        "# Starting dates for validation and test data\n",
        "valid_start_dt = \"1997-09-28\"\n",
        "\n",
        "forecast_for_kaggle = True\n",
        "if forecast_for_kaggle:\n",
        "    test_start_dt = \"1998-03-23\"\n",
        "else:\n",
        "    test_start_dt = dt.datetime.strptime(\"1998-03-23\", \"%Y-%m-%d\") - dt.timedelta(days=HORIZON)\n",
        "    test_start_dt = test_start_dt.strftime(\"%Y-%m-%d\")\n",
        "    DT_test = DT_train.tail(HORIZON).copy()\n",
        "    DT_train = DT_train.head(-HORIZON).copy()\n",
        "\n",
        "last_day_train = DT_train.index[-1]\n",
        "test_dates = pd.date_range(start=last_day_train, periods=HORIZON + 1)[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmfOzzQyEqfJ"
      },
      "source": [
        "`use_subset_of_series` restricts the number of series to 2 for faster results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuqcRnQasf7x"
      },
      "outputs": [],
      "source": [
        "use_subset_of_series = True\n",
        "if use_subset_of_series:\n",
        "    id_series_all = DT_train.columns[:2]\n",
        "else:\n",
        "    id_series_all = DT_train.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwXalqiaFQ3h"
      },
      "source": [
        "## Seasonal naive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpz0G0Jvsf7y"
      },
      "outputs": [],
      "source": [
        "period = 7\n",
        "T = len(DT_train)\n",
        "\n",
        "fcts_snaive_list = list()\n",
        "\n",
        "for id_series in id_series_all:\n",
        "    series_train = DT_train[id_series]\n",
        "    f_snaive = [series_train[T + h - period * ((HORIZON - 1) // period + 1)] for h in range(0, HORIZON)]\n",
        "    f_snaive = pd.Series(f_snaive, index=test_dates)\n",
        "    f_snaive.name = id_series\n",
        "    fcts_snaive_list.append(f_snaive)\n",
        "\n",
        "fcts_snaive = pd.concat(fcts_snaive_list, axis=1)\n",
        "\n",
        "kaggle_submission_naive = kaggle_forecasts(fcts_snaive)\n",
        "kaggle_submission_naive.to_csv(\"../work/submission_snaive.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjUr7MHAsf7y"
      },
      "source": [
        "## AutoArima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn0b9I61sf7z"
      },
      "outputs": [],
      "source": [
        "from pmdarima.arima import auto_arima\n",
        "\n",
        "fcts_arima_list = list()\n",
        "\n",
        "for id_series in id_series_all:\n",
        "    print(\"======\", id_series, \"======\")\n",
        "    y = DT_train[id_series]\n",
        "    model = auto_arima(\n",
        "        y,\n",
        "        d=0,\n",
        "        start_p=0,\n",
        "        max_p=1,\n",
        "        start_q=0,\n",
        "        max_q=1,\n",
        "        D=0,\n",
        "        start_P=0,\n",
        "        max_P=1,\n",
        "        start_Q=0,\n",
        "        max_Q=0,\n",
        "        m=7,\n",
        "        trace=False,\n",
        "    )\n",
        "    f_arima = model.predict(HORIZON)\n",
        "    f_arima.name = id_series\n",
        "    fcts_arima_list.append(f_arima)\n",
        "\n",
        "fcts_arima = pd.concat(fcts_arima_list, axis=1)\n",
        "\n",
        "kaggle_submission_arima = kaggle_forecasts(fcts_arima)\n",
        "kaggle_submission_arima.to_csv(\"../work/submission_arima.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB7-5zw5sf7z"
      },
      "outputs": [],
      "source": [
        "from main.utils.utils_methods import embed_data, plot_learning_curves\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import (\n",
        "    Dense,\n",
        "    GRU,\n",
        "    SimpleRNN,\n",
        "    LSTM,\n",
        "    Conv1D,\n",
        "    Flatten,\n",
        "    RepeatVector,\n",
        "    TimeDistributed,\n",
        "    Flatten,\n",
        "    Input,\n",
        ")\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP9R_nBUsf7z"
      },
      "source": [
        "## Multilayer Perceptron (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHbUdzojsf7z"
      },
      "outputs": [],
      "source": [
        "def mlp(X_train, y_train, X_valid, y_valid, best_val, **kwargs):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(kwargs[\"LATENT_DIM\"], activation=\"relu\", input_shape=(kwargs[\"LAG\"],)))\n",
        "    model.add(Dense(kwargs[\"output_size\"]))\n",
        "\n",
        "    model.compile(optimizer=kwargs[\"optimizer\"], loss=kwargs[\"loss\"])\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        batch_size=kwargs[\"BATCH_SIZE\"],\n",
        "        epochs=kwargs[\"EPOCHS\"],\n",
        "        validation_data=(X_valid, y_valid),\n",
        "        callbacks=[kwargs[\"earlystop\"], best_val],\n",
        "        verbose=kwargs[\"verbose\"],\n",
        "    )\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umDZtWYisf7z"
      },
      "source": [
        "## 1-D Convolutional Neural Networks (CNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJmJruuTsf70"
      },
      "outputs": [],
      "source": [
        "def cnn_dilated(train_inputs, valid_inputs, best_val, **kwargs):\n",
        "    model = Sequential()\n",
        "    model.add(\n",
        "        Conv1D(\n",
        "            kwargs[\"LATENT_DIM\"],\n",
        "            kernel_size=kwargs[\"KERNEL_SIZE\"],\n",
        "            padding=\"causal\",\n",
        "            strides=1,\n",
        "            activation=\"relu\",\n",
        "            dilation_rate=1,\n",
        "            input_shape=(kwargs[\"LAG\"], 1),\n",
        "        )\n",
        "    )\n",
        "    model.add(\n",
        "        Conv1D(\n",
        "            kwargs[\"LATENT_DIM\"],\n",
        "            kernel_size=kwargs[\"KERNEL_SIZE\"],\n",
        "            padding=\"causal\",\n",
        "            strides=1,\n",
        "            activation=\"relu\",\n",
        "            dilation_rate=2,\n",
        "        )\n",
        "    )\n",
        "    model.add(\n",
        "        Conv1D(\n",
        "            kwargs[\"LATENT_DIM\"],\n",
        "            kernel_size=kwargs[\"KERNEL_SIZE\"],\n",
        "            padding=\"causal\",\n",
        "            strides=1,\n",
        "            activation=\"relu\",\n",
        "            dilation_rate=4,\n",
        "        )\n",
        "    )\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(kwargs[\"output_size\"], activation=\"linear\"))\n",
        "\n",
        "    model.compile(optimizer=kwargs[\"optimizer\"], loss=kwargs[\"loss\"])\n",
        "    history = model.fit(\n",
        "        train_inputs[\"encoder_input\"],\n",
        "        train_inputs[\"target\"],\n",
        "        batch_size=kwargs[\"BATCH_SIZE\"],\n",
        "        epochs=kwargs[\"EPOCHS\"],\n",
        "        validation_data=(valid_inputs[\"encoder_input\"], valid_inputs[\"target\"]),\n",
        "        callbacks=[kwargs[\"earlystop\"], best_val],\n",
        "        verbose=kwargs[\"verbose\"],\n",
        "    )\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7V2Cq79sf70"
      },
      "source": [
        "## RNN vector-output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeecizRTsf70"
      },
      "outputs": [],
      "source": [
        "def rnn_vector_output(train_inputs, valid_inputs, best_val, **kwargs):\n",
        "    model = Sequential()\n",
        "    model.add(kwargs[\"RECURRENT_MODEL\"](kwargs[\"LATENT_DIM\"], input_shape=(kwargs[\"LAG\"], 1)))\n",
        "    model.add(Dense(kwargs[\"output_size\"]))\n",
        "\n",
        "    model.compile(optimizer=kwargs[\"optimizer\"], loss=kwargs[\"loss\"])\n",
        "\n",
        "    history = model.fit(\n",
        "        train_inputs[\"encoder_input\"],\n",
        "        train_inputs[\"target\"],\n",
        "        batch_size=kwargs[\"BATCH_SIZE\"],\n",
        "        epochs=kwargs[\"EPOCHS\"],\n",
        "        validation_data=(valid_inputs[\"encoder_input\"], valid_inputs[\"target\"]),\n",
        "        callbacks=[kwargs[\"earlystop\"], best_val],\n",
        "        verbose=kwargs[\"verbose\"],\n",
        "    )\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSo9mTm8sf70"
      },
      "source": [
        "## RNN encoder-decoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA_votwCsf70"
      },
      "outputs": [],
      "source": [
        "def rnn_encoder_decoder(train_inputs, valid_inputs, best_val, **kwargs):\n",
        "    model = Sequential()\n",
        "    model.add(kwargs[\"RECURRENT_MODEL\"](kwargs[\"LATENT_DIM\"], input_shape=(kwargs[\"LAG\"], 1)))\n",
        "    model.add(RepeatVector(kwargs[\"output_size\"]))\n",
        "    model.add(kwargs[\"RECURRENT_MODEL\"](kwargs[\"LATENT_DIM\"], return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(1)))\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.compile(optimizer=kwargs[\"optimizer\"], loss=kwargs[\"loss\"])\n",
        "\n",
        "    history = model.fit(\n",
        "        train_inputs[\"encoder_input\"],\n",
        "        train_inputs[\"target\"],\n",
        "        batch_size=kwargs[\"BATCH_SIZE\"],\n",
        "        epochs=kwargs[\"EPOCHS\"],\n",
        "        validation_data=(valid_inputs[\"encoder_input\"], valid_inputs[\"target\"]),\n",
        "        callbacks=[kwargs[\"earlystop\"], best_val],\n",
        "        verbose=kwargs[\"verbose\"],\n",
        "    )\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5s-Ggomsf70"
      },
      "source": [
        "## Data transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMvMjlbmsf71"
      },
      "outputs": [],
      "source": [
        "log_transformation = True\n",
        "clean_series = True\n",
        "\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "\n",
        "for id_series in id_series_all:\n",
        "    print(\"======\", id_series, \"======\")\n",
        "\n",
        "    # Log transformation\n",
        "    if log_transformation:\n",
        "        series = DT_train[id_series]\n",
        "        series = np.log1p(series)\n",
        "        DT_train[id_series] = series\n",
        "\n",
        "    if clean_series:\n",
        "        # Replace zeroes\n",
        "        series = DT_train[id_series]\n",
        "        series[series == 0] = pd.NA\n",
        "        series.replace(0, value=pd.NA, inplace=True)\n",
        "        series.fillna(method=\"backfill\", inplace=True)\n",
        "        # Season-Trend decomposition using LOESS\n",
        "        stl = STL(series, period=7, robust=True, seasonal=7 * 50 + 1, trend=7 * 50 + 1)\n",
        "        result = stl.fit()\n",
        "        # Remove outliers in the residuals\n",
        "        e = result.resid.copy()\n",
        "        q_l, q_u = np.quantile(e, [0.05, 0.95])\n",
        "        e[e <= q_l] = pd.NA\n",
        "        e[e >= q_u] = pd.NA\n",
        "        e.fillna(method=\"backfill\", inplace=True)\n",
        "        series = result.trend + result.seasonal + e\n",
        "\n",
        "        DT_train[id_series] = series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GAufs3dKJ9c"
      },
      "source": [
        "## Forecast decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWHw3Fdbsf71"
      },
      "source": [
        "* Do we forecast the raw series? If not, we first decompose the series into trend, seasonal and remainder components. Then, we forecast each components independently. Alternatively, we can combine the trend and remainder components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YxjTJt7sf71"
      },
      "outputs": [],
      "source": [
        "forecast_by_decomposition = True\n",
        "combine_trend_and_remainder = False\n",
        "\n",
        "if forecast_by_decomposition:\n",
        "    fcts_trend_list = list()\n",
        "    fcts_seas_list = list()\n",
        "    for id_series in id_series_all:\n",
        "        print(\"======\", id_series, \"======\")\n",
        "        series = DT_train[id_series]\n",
        "        stl = STL(series, period=7, robust=True, seasonal=7 * 50 + 1, trend=7 * 50 + 1)\n",
        "        result = stl.fit()\n",
        "        if use_subset_of_series:\n",
        "            result.plot()\n",
        "            plt.show()\n",
        "\n",
        "        trend_component = result.trend\n",
        "        seasonal_component = result.seasonal\n",
        "        remainder_component = result.resid\n",
        "\n",
        "        if not combine_trend_and_remainder:\n",
        "            # Forecast the trend\n",
        "            from statsmodels.tsa.api import SimpleExpSmoothing\n",
        "\n",
        "            fit_ses = SimpleExpSmoothing(trend_component, initialization_method=\"estimated\").fit()\n",
        "            pred_trend = fit_ses.forecast(HORIZON)\n",
        "            fcts_trend_list.append(pred_trend)\n",
        "\n",
        "            # Remainder\n",
        "            DT_train[id_series] = remainder_component\n",
        "        else:\n",
        "            DT_train[id_series] = trend_component + remainder_component\n",
        "\n",
        "        # Forecast the seasonality\n",
        "        pred_seasonal = seasonal_component.tail(HORIZON)\n",
        "        fcts_seas_list.append(pred_seasonal)\n",
        "\n",
        "    if not combine_trend_and_remainder:\n",
        "        fcts_trend = pd.concat(fcts_trend_list, axis=1)\n",
        "        fcts_trend.columns = id_series_all\n",
        "\n",
        "    fcts_seas = pd.concat(fcts_seas_list, axis=1)\n",
        "    fcts_seas.columns = id_series_all\n",
        "    fcts_seas.index = test_dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKfG_5WKsf71"
      },
      "outputs": [],
      "source": [
        "# Add rows to easily build X_test.\n",
        "\n",
        "DT_train.shape[1]\n",
        "\n",
        "DT_pad = DT_train.tail(HORIZON).copy()\n",
        "DT_pad.index = test_dates\n",
        "DT_pad[:] = np.inf\n",
        "DT = pd.concat([DT_train, DT_pad])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fKg2ku3sf71"
      },
      "source": [
        "## Hyperparameter configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv7Dn2_Ssf71"
      },
      "outputs": [],
      "source": [
        "optimizer_adam = keras.optimizers.Adam(learning_rate=0.01)  # optimizer_rmsprop = 'RMSprop'\n",
        "earlystop = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=50, restore_best_weights=True)\n",
        "verbose = 0\n",
        "\n",
        "# Loss function to be used to optimize the model parameters\n",
        "loss_fct = \"mae\"  # 'mae'\n",
        "\n",
        "LAG = 8  # 4\n",
        "params_mlp_rec = {\n",
        "    \"LATENT_DIM\": 5,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"EPOCHS\": 100,\n",
        "    \"LAG\": LAG,\n",
        "    \"output_size\": 1,\n",
        "    \"loss\": loss_fct,\n",
        "    \"optimizer\": optimizer_adam,\n",
        "    \"earlystop\": earlystop,\n",
        "    \"verbose\": verbose,\n",
        "}\n",
        "params_mlp_mo = {\n",
        "    \"LATENT_DIM\": 5,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"EPOCHS\": 100,\n",
        "    \"LAG\": LAG,\n",
        "    \"output_size\": HORIZON,\n",
        "    \"loss\": loss_fct,\n",
        "    \"optimizer\": optimizer_adam,\n",
        "    \"earlystop\": earlystop,\n",
        "    \"verbose\": verbose,\n",
        "}\n",
        "params_cnn = {\n",
        "    \"KERNEL_SIZE\": 10,\n",
        "    \"LATENT_DIM\": 5,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"EPOCHS\": 100,\n",
        "    \"LAG\": LAG,\n",
        "    \"output_size\": HORIZON,\n",
        "    \"loss\": loss_fct,\n",
        "    \"optimizer\": optimizer_adam,\n",
        "    \"earlystop\": earlystop,\n",
        "    \"verbose\": verbose,\n",
        "}\n",
        "params_rnn = {\n",
        "    \"RECURRENT_MODEL\": GRU,\n",
        "    \"LATENT_DIM\": 5,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"EPOCHS\": 100,\n",
        "    \"LAG\": LAG,\n",
        "    \"output_size\": HORIZON,\n",
        "    \"loss\": loss_fct,\n",
        "    \"optimizer\": optimizer_adam,\n",
        "    \"earlystop\": earlystop,\n",
        "    \"verbose\": verbose,\n",
        "}\n",
        "# RECURRENT_MODEL:  SimpleRNN or GRU or LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmGFKBnIsf71"
      },
      "source": [
        "## Model training and forecasting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uENtjllhsf71"
      },
      "outputs": [],
      "source": [
        "enable_plots = False\n",
        "\n",
        "methods = [\"mlp_recursive\", \"mlp_multioutput\", \"cnn\", \"rnn_vector_output\", \"rnn_encoder_decoder\"]\n",
        "\n",
        "forecasts_lists = {\n",
        "    method: []\n",
        "    for method in methods\n",
        "}\n",
        "\n",
        "# Useful to build X_valid and X_test\n",
        "look_back_valid = dt.datetime.strptime(valid_start_dt, \"%Y-%m-%d\") - dt.timedelta(days=LAG)\n",
        "look_back_test = dt.datetime.strptime(test_start_dt, \"%Y-%m-%d\") - dt.timedelta(days=LAG)\n",
        "\n",
        "for id_series in id_series_all:\n",
        "    print(id_series)\n",
        "\n",
        "    ##TRAINING/VALIDATION/TEST DATA\n",
        "    train = DT.copy()[DT.index < valid_start_dt][[id_series]]\n",
        "    valid = DT.copy()[(DT.index >= look_back_valid) & (DT.index < test_start_dt)][[id_series]]\n",
        "    test = DT.copy()[(DT.index >= look_back_test)][[id_series]]\n",
        "    train_inputs, valid_inputs, test_inputs, X_train, y_train, X_valid, y_valid, X_test, y_test = embed_data(\n",
        "        train, valid, test, HORIZON, LAG, variable=id_series\n",
        "    )\n",
        "\n",
        "    for method in methods:\n",
        "        file_header = \"model_\" + method + \"_\" + id_series\n",
        "        best_val = ModelCheckpoint(\n",
        "            \"../work/\" + file_header + \"_{epoch:02d}.h5\",\n",
        "            save_best_only=True,\n",
        "            mode=\"min\",\n",
        "            save_freq=\"epoch\",\n",
        "            monitor=\"val_loss\",\n",
        "        )\n",
        "        if method == \"mlp_recursive\":\n",
        "            _, _, _, X_train_onestep, y_train_onestep, X_valid_onestep, y_valid_onestep, _, _ = embed_data(\n",
        "                train, valid, test, 1, LAG, freq=None, variable=id_series\n",
        "            )\n",
        "            model, history_loss = mlp(\n",
        "                X_train_onestep,\n",
        "                y_train_onestep,\n",
        "                X_valid_onestep,\n",
        "                y_valid_onestep,\n",
        "                best_val=best_val,\n",
        "                **params_mlp_rec\n",
        "            )\n",
        "        elif method == \"mlp_multioutput\":\n",
        "            model, history_loss = mlp(X_train, y_train, X_valid, y_valid, best_val=best_val, **params_mlp_mo)\n",
        "        elif method == \"cnn\":\n",
        "            model, history_loss = cnn_dilated(train_inputs, valid_inputs, best_val=best_val, **params_cnn)\n",
        "        elif method == \"rnn_vector_output\":\n",
        "            model, history_loss = rnn_vector_output(\n",
        "                train_inputs, valid_inputs, best_val=best_val, **params_rnn\n",
        "            )\n",
        "        elif method == \"rnn_encoder_decoder\":\n",
        "            model, history_loss = rnn_encoder_decoder(\n",
        "                train_inputs, valid_inputs, best_val=best_val, **params_rnn\n",
        "            )\n",
        "\n",
        "        if enable_plots:\n",
        "            plot_learning_curves(history_loss, title=id_series + \" - \" + method)\n",
        "\n",
        "        best_epoch = np.argmin(np.array(history_loss.history[\"val_loss\"])) + 1\n",
        "        filepath = \"../work/\" + file_header + \"_{:02d}.h5\"\n",
        "        model.load_weights(filepath.format(best_epoch))\n",
        "\n",
        "        # Delete files (generate too many files)\n",
        "        for filename in glob.glob(\"../work/*\" + file_header + \"*\"):\n",
        "            os.remove(filename)\n",
        "\n",
        "        ## Forecasting\n",
        "        if method == \"mlp_recursive\":\n",
        "            for h in range(HORIZON):\n",
        "                pred = model.predict(X_test, verbose=0)\n",
        "                X_test = pd.DataFrame(\n",
        "                    np.hstack((np.delete(X_test.to_numpy(), 0, 1), pred)),\n",
        "                    index=X_test.index,\n",
        "                    columns=X_test.columns,\n",
        "                )\n",
        "                if h > 0:\n",
        "                    predictions = np.hstack((predictions, pred))\n",
        "                else:\n",
        "                    predictions = pred\n",
        "            forecasts_lists[method].append(pd.DataFrame(predictions).T)\n",
        "        elif method == \"mlp_multioutput\":\n",
        "            predictions = model.predict(X_test, verbose=0)\n",
        "            forecasts_lists[method].append(pd.DataFrame(predictions).T)\n",
        "        elif method in [\"cnn\", \"rnn_vector_output\", \"rnn_encoder_decoder\"]:\n",
        "            predictions = model.predict(test_inputs[\"encoder_input\"], verbose=0)\n",
        "            if method == \"cnn\":\n",
        "                forecasts_lists[method].append(pd.DataFrame(predictions).T)\n",
        "            elif method == \"rnn_vector_output\":\n",
        "                forecasts_lists[method].append(pd.DataFrame(predictions).T)\n",
        "            elif method == \"rnn_encoder_decoder\":\n",
        "                forecasts_lists[method].append(pd.DataFrame(predictions).T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO0-echOsf72"
      },
      "outputs": [],
      "source": [
        "def make_forecast_from_list(forecast_list):\n",
        "    forecast = pd.concat(forecast_list, axis=1)\n",
        "    forecast.columns = id_series_all\n",
        "    forecast.index = test_dates\n",
        "    return forecast\n",
        "\n",
        "forecasts = {\n",
        "    method: make_forecast_from_list(forecasts_list)\n",
        "    for method, forecasts_list in forecasts_lists.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4EMpFqPKWii"
      },
      "source": [
        "## Inverse data transformation and decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhrpYw6Hsf72"
      },
      "outputs": [],
      "source": [
        "def inverse_transform(fct):\n",
        "    if forecast_by_decomposition:\n",
        "        fct += fcts_seas\n",
        "        if not combine_trend_and_remainder:\n",
        "            fct += fcts_trend\n",
        "    if log_transformation:\n",
        "        fct = np.expm1(fct)\n",
        "    return fct\n",
        "\n",
        "forecasts = {\n",
        "    method: inverse_transform(fct)\n",
        "    for method, fct in forecasts.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYe8XnUAvBWE"
      },
      "outputs": [],
      "source": [
        "fcts_zero = pd.DataFrame(0, index=fcts_seas.index, columns=fcts_seas.columns)\n",
        "fcts_zero = inverse_transform(fcts_zero)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp1Tts8Usf72"
      },
      "outputs": [],
      "source": [
        "def smape(y_true, y_pred):\n",
        "    assert (y_pred >= 0).all().all()\n",
        "    denominator = (y_true + y_pred) / 200.0\n",
        "    SAPE = np.abs(y_true - y_pred) / denominator\n",
        "    SAPE[denominator == 0] = 0.0\n",
        "    return SAPE.mean().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvGSV2uhOogX"
      },
      "outputs": [],
      "source": [
        "for method, fcts in forecasts.items():\n",
        "    kaggle_submission = kaggle_forecasts(fcts)\n",
        "    kaggle_submission.to_csv(f\"../work/submission_{method}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErruIEh2NqdO"
      },
      "source": [
        "## Test procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOqu6g66sf72"
      },
      "outputs": [],
      "source": [
        "kaggle_test_available = os.path.exists(\"../data/private\")\n",
        "\n",
        "if forecast_for_kaggle and kaggle_test_available:\n",
        "    print(\"Reading file\")\n",
        "    DT_test = pd.read_csv(\"../data/private/test.csv\", parse_dates=True)\n",
        "    DT_test[\"Day\"] = pd.to_datetime(DT_test[\"Day\"], format=\"%Y-%m-%d\")\n",
        "    DT_test.set_index(\"Day\", inplace=True)\n",
        "    DT_test = DT_test.asfreq(\"D\")\n",
        "    DT_test.head()\n",
        "\n",
        "if not forecast_for_kaggle or kaggle_test_available:\n",
        "    DT_test_sub = DT_test[id_series_all]\n",
        "    print(smape(DT_test_sub, fcts_snaive))\n",
        "    print(smape(DT_test_sub, fcts_arima))\n",
        "\n",
        "    for method, fcts in forecasts.items():\n",
        "        print(smape(DT_test_sub, fcts))\n",
        "\n",
        "    # DT_test_sub[\"s001\"].plot()\n",
        "    # fcts_snaive[\"s001\"].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBe0oLdb_oXn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
